#!/usr/bin/env python
import os
import re
import sys
import argparse
import json
import time
import fileinput
import itertools
import collections
import math
import shlex
import signal
import select
import Queue
import subprocess as sp
import multiprocessing as mp

import flux
import flux.kvs as kvs
import flux.kz as kz

arg_parser = argparse.ArgumentParser(
    description="Execute a group of programs with the flux resource manager")

arg_parser.add_argument('--tasks', '-n', type=int, default=1)
arg_parser.add_argument('--nodes', '-N', type=int, default=0)
arg_parser.add_argument('--scheduler', '-S')
arg_parser.add_argument('--prefix', '-P', help="Prefix output/err lines with the associated job id", action='store_true')


arg_parser.add_argument(
    '--command_file', '-c',
    action='append',
    default=[],
    help='''Name of a file with one argument list per line, blank
lines and lines where the first non-whitespace character is a # are ignored,
all arguments accepted on the %(prog)s line are accepted in the file, may
be specified multiple times''')

try: 
    import cram
    arg_parser.add_argument(
        '--cram_file', '-C',
        action='append',
        default=[],
        help='''Name of a command file created by the CRAM utility''')
except ImportError:
    cram = False

arg_parser.add_argument('command', nargs=argparse.REMAINDER)

args = arg_parser.parse_args(sys.argv[1:])


not_done = True
max_pending = 0
max_running = 0


Core = collections.namedtuple('Core', ['rank_id', 'os_index', 'logical_index'])
Rank = collections.namedtuple('Rank', ['rank_id', 'cores', 'available_cores'])

class Scheduler(object):
    """ A wrapper API for scheduler operations, mainly to stub out
    functionality that currently does not work without it."""

    def __init__(self, watcher_stream, flux_handle = None, interface = None):
        if flux_handle is None:
            flux_handle = flux.Flux()
        self.total_ranks = 0
        self.total_cores = 0
        self.watcher_stream = watcher_stream
        self.interface = interface
        self.fh = flux_handle
        self.ranks = {}
        self.available_ranks = {}
        self.jobs = {}
        self.pending_jobs = collections.OrderedDict()
        self.running_jobs = collections.OrderedDict()
        self.completed_jobs = collections.OrderedDict()

        self.read_resource_availability()

    def get_os_id_for_core(self, kd, index):
        # TODO: bring this functionality back, but faster, for now the
        # specific ID is unnecessary due to lack of binding anyway
        # for root, dirs, files in kvs.walk(kd, topdown=True):
        #     if re.match('.*Core_{}$'.format(index), root):
        #         return kd[kvs.join(root, 'os_index')]
        return index
        # raise RuntimeError("Hardware description for core {} under {} not found".format(index, kd))

    def read_resource_availability(self):
        rank_resources = kvs.get_dir(self.fh, 'resource.hwloc.by_rank')
        for rank_id, resources in rank_resources.items():
            cores = []
            for i in range(0, resources['Core']):
                os_index = self.get_os_id_for_core(resources, i)
                c = Core(int(rank_id), os_index, i)
                cores.append(c)
                self.total_cores += 1
            r = Rank(int(rank_id), cores, cores)
            self.ranks[int(rank_id)] = r
            self.available_ranks[int(rank_id)] = r
            self.total_ranks += 1

    def check_feasibility(self, job):
        if job.spec['nnodes'] > len(self.ranks):
            raise RuntimeError(
                "More resources have been requested than exist: {}".format(job))

    def submit(self, job):
        self.check_feasibility(job)
        resp = self.fh.rpc_send('job.create', json.dumps(job.spec))
        if resp is None:
            raise RuntimeError("RPC response invalid")
        if resp.get('errnum', None) is not None:
            raise RuntimeError("Job creation failed with error code {}".format(
                resp['errnum']))
        job.job_id = resp['jobid']
        self.jobs[int(job.job_id)] = job
        jobdir = 'lwj.{}'.format(job.job_id)
        job.kvs = kvs.get_dir(self.fh, jobdir)
        self.watcher_stream.put((job.job_id, job.spec['ntasks']))
        self.watcher_stream.join()
        if self.interface is not None:
            self.interface.submitted(job)
        self.schedule(job)

    def release_job_resources(self, job):
        pass

    def try_schedule(self):
        pending = self.pending_jobs
        self.pending_jobs = collections.OrderedDict()

        # TODO: this will have pathologically bad performance under some cases, rework
        for jid, j in pending.items():
            self.schedule(j)

    def state_change(self, key, state):
        # print "received state change to: '{}'".format(state), key, state, state == 'complete'
        m = re.match('lwj.(\d*).state', key)
        job_id = int(m.group(1))
        self.jobs[job_id].state = state
        if self.interface is not None:
            self.interface.state_changed(self.jobs[job_id], state + " pending: {} running: {}".format(len(self.pending_jobs), len(self.running_jobs)))
        if state == 'running':
            job = self.pending_jobs.pop(job_id, None)
            if job is not None:
                self.running_jobs[job_id] = job
        if state == 'complete':
            job = self.running_jobs.pop(job_id)
            self.completed_jobs[job_id] = job
            self.release_job_resources(job)
            if len(self.pending_jobs) == 0 and len(self.running_jobs) == 0:
                not_done = False
                return 0

            if len(self.pending_jobs) > 0:
                self.try_schedule()
        return 0

    def schedule(self, job):
        pass

class FluxScheduler(Scheduler):
    """ Interact with the flux-sched module in the instance """

    def __init__(self, watcher_stream, flux_handle = None, interface = None):
        super(FluxScheduler, self).__init__(watcher_stream, flux_handle=flux_handle, interface = interface)

    def schedule(self, job):
        super(FluxScheduler, self).schedule(job)
        self.pending_jobs[int(job.job_id)] = job
        # TODO: this should be subsumed by a job submission and event
        # interface
        job.kvs['state'] = 'submitted'
        job.kvs.commit()

class SlurmScheduler(Scheduler):
    """ Interact with the flux-sched module in the instance """

    def __init__(self, watcher_stream, flux_handle = None, interface = None):
        super(SlurmScheduler, self).__init__(watcher_stream, flux_handle=flux_handle, interface = interface)

    def schedule(self, job):
        super(SlurmScheduler, self).schedule(job)
        self.pending_jobs[int(job.job_id)] = job
        # TODO: this should be subsumed by a job submission and event
        # interface
        job.kvs['state'] = 'submitted'
        job.kvs.commit()

class CapScheduler(Scheduler):
    """ Inner scheduler for testing while working on FluxScheduler support """
    def __init__(self, watcher_stream, flux_handle = None, interface = None):
        super(CapScheduler, self).__init__(watcher_stream, flux_handle=flux_handle, interface = interface)
        self.partial_ranks = {}

    def schedule(self, job):
        global max_running
        global max_pending
        super(CapScheduler, self).schedule(job)
        if job.spec['nnodes'] > len(self.available_ranks):
            self.pending_jobs[int(job.job_id)] = job
            max_pending = max_pending if max_pending > len(self.pending_jobs) else len(self.pending_jobs)
            return

        assigned_ranks = []
        assigned_cores = []
        # One or more nodes has been requested, spread tasks across nodes,
        # nodes are not uniquely allocated unless fully subscribed
        if job.spec['nnodes'] > 0:
            tasks_per_node = int(job.spec['ntasks'] / job.spec['nnodes'])
            remainder = job.spec['ntasks'] % job.spec['nnodes']
            while len(assigned_ranks) < job.spec['nnodes']:
                tasks = tasks_per_node
                if remainder > 0:
                    tasks += 1
                    remainder -= 1
                rid, r = self.available_ranks.popitem()
                assigned_ranks.append(r)
                while len(r.available_cores):
                    core = r.available_cores.pop()
                job.kvs['rank.{}.cores'.format(r.rank_id)] = tasks
        else:
            tasks_required = job.spec['ntasks']
            cores_required = job.spec['ntasks']
            if cores_required > self.total_cores:
                cores_required = self.total_cores
            for rid, r in self.partial_ranks.items():
                cores_from_here = 0
                while len(r.available_cores):
                    c = r.available_cores.pop()
                    assigned_cores.append(c)
                    cores_from_here += 1
                    if len(assigned_cores) >= cores_required:
                        break
                if len(r.available_cores) == 0:
                    del self.partial_ranks[rid]
                if cores_from_here > 0:
                    job.kvs['rank.{}.cores'.format(r.rank_id)
                            ] = cores_from_here
                if len(assigned_cores) >= cores_required:
                    break
            if len(assigned_cores) < cores_required:
                for rid, r in self.available_ranks.items():
                    cores_from_here = 0
                    while len(r.available_cores):
                        c = r.available_cores.pop()
                        assigned_cores.append(c)
                        cores_from_here += 1
                        if len(assigned_cores) >= cores_required:
                            break
                    if len(r.available_cores):
                        self.partial_ranks[rid] = r

                    del self.available_ranks[rid]

                    if cores_from_here > 0:
                        allocate_count = cores_from_here
                        if tasks_required > cores_required:
                            # Allocate by multiples
                            modifier = int(math.floor(tasks_required / cores_required))
                            allocate_count *= modifier
                            if tasks_required % cores_required > 0:
                                # Clean up the stragglers
                                allocate_count += 1
                                tasks_required -= 1
                            cores_from_here = allocate_count

                        job.kvs['rank.{}.cores'.format(r.rank_id)] = cores_from_here
                    if len(assigned_cores) >= cores_required:
                        break
            if len(assigned_cores) < cores_required:
                job.resources = {
                    'ranks': assigned_ranks,
                    'cores': assigned_cores
                }
                # Not envough cores available
                self.release_job_resources(job)
                self.pending_jobs[int(job.job_id)] = job
                max_pending = max_pending if max_pending > len(self.pending_jobs) else len(self.pending_jobs)
                return

        job.resources = {'ranks': assigned_ranks, 'cores': assigned_cores}
        job.kvs.commit()
        self.fh.event_send('wrexec.run.{}'.format(job.job_id))
        self.running_jobs[job.job_id] = job
        max_running = max_running if max_running > len(self.running_jobs) else len(self.running_jobs)


    def release_job_resources(self, job):
        for r in job.resources['ranks']:
            r.available_cores.extend(r.cores)
        for c in job.resources['cores']:
            r = self.ranks[c.rank_id]
            r.available_cores.append(c)
            if len(r.available_cores) == len(r.cores):
                self.available_ranks[r.rank_id] = r
                try:
                    del self.partial_ranks[r.rank_id]
                except:
                    pass

env_filter = re.compile(r"^(SLURM_|FLUX_)")

def get_environment():
    env = dict()
    keys = env.keys() #inefficient, but effective
    for key in os.environ:
        if env_filter.match(key):
           continue
        env[key] = os.environ[key]
    env.pop('HOSTNAME', None)
    env.pop('ENVIRONMENT', None)
    # Make MVAPICH behave...
    env['MPIRUN_RSH_LAUNCH'] = '1'
    return env

class Job(object):
    def __init__(self, command_arr, **kwargs):
        self.job_id = None
        self.kvs = None
        self.state_count = 0
        self.spec = {
            'nnodes': 0,
            'ntasks': 1,
            'cmdline': command_arr,
            'environ': get_environment(),  # TODO: filter this
            'cwd': os.getcwd(),
        }
        if len(kwargs):
            self.spec.update(kwargs)

        self.pre_validate()

    def pre_validate(self):
        if self.spec['ntasks'] < self.spec['nnodes']:
            self.spec['ntasks'] = self.spec['nnodes']

        if self.spec['ntasks'] <= 0:
            raise AttributeError("A program must contain at least one task")

    def __str__(self):
        if self.job_id is None:
            return "<Job, unwritten: " + str(self.spec) + ">"
        elif self.kvs is None:
            return "<Job, unsubmitted: " + str(self.spec) + ">"
        else:
            ret = "Submitted job, kvs contents:\n"
            for root, dirs, files in kvs.walk(self.kvs, topdown=True):
                for f in files:
                    ret = ret + "{}.{}={}\n".format(
                        root, f, self.kvs[kvs.join(root, f)])
            return ret

class UserInterface(object):
    def __init__(self, stream=None):
        self.stream = stream
        if self.stream is None:
            self.stream = sys.stdout

    def state_changed(self, job, state):
        print >> self.stream, "job changed state", job.job_id, state

    def submitted(self, job):
        print >> self.stream, "job submitted", job.job_id, "pending"



def scheduler_proc(job_stream, watcher_stream, state_stream, finish):
    ret = 0
    interface = UserInterface()
    if args.scheduler:
        sched = FluxScheduler(watcher_stream, interface=interface)
    else:
        sched = CapScheduler(watcher_stream, interface=interface)
    closed = False
    while not_done:
        reader_list = [  state_stream._reader ]
        if len(sched.pending_jobs) < 100:
            reader_list.append(job_stream._reader)

        to_read, [], [] = select.select(reader_list, [], [])
        for fd in to_read:
            if fd == job_stream._reader:
                j = job_stream.get()
                if j == "DONE":
                    closed = True
                    continue
                sched.submit(j)
            elif fd == state_stream._reader:
                while True:
                    try:
                        (key, value) = state_stream.get_nowait()
                        sched.state_change(key, value)
                    except Queue.Empty:
                        break
            else:
                raise RuntimeError("Unknown stream returned from select")
        if len(sched.pending_jobs) == len(sched.running_jobs) == 0 and closed:
            break

    if len(sched.pending_jobs) + len(sched.running_jobs) == 0:
        print "All jobs complete, exiting"
    else:
        print "Some jobs did not finish"
    watcher_stream.put("DIE")

depth = 0
def run_args(cur_args, job_stream):
    global depth
    commands = None
    # iterate over all commands from all sources
    if len(cur_args.command) > 0:
        j = Job(cur_args.command, ntasks=cur_args.tasks, nnodes=cur_args.nodes)
        job_stream.put(j)
    elif len(cur_args.command_file) > 0:
        depth += 1
        commands = fileinput.FileInput(cur_args.command_file)
        for i, c in enumerate(commands):
            arg_list = shlex.split(c, comments=True)
            if len(arg_list) == 0:
                continue
            local_args = arg_parser.parse_args(arg_list)
            run_args(local_args, job_stream)
    elif cram and len(cur_args.cram_file) > 0:
        depth += 1
        for cfile in cur_args.cram_file:
            for c in cram.cramfile.CramFile(cfile):
                if len(c.args) == 0:
                    continue
                job_stream.put(Job(
                    c.args,
                    ntasks=c.num_procs,
                    environ=c.env,
                    cwd=c.working_dir,
                    ))
    elif depth == 0:
        # If there are still no commands, attempt to read them from standard input
        raise RuntimeError("No commands specified")


def command_supplier_proc(job_stream):
    run_args(args, job_stream)
    job_stream.put("DONE")

def event_reactor_proc(watch_stream, state_stream):
    fh = flux.Flux()
    def job_state_change(key, value, scheduler, errnum):
        if value == 'complete':
            kvs.unwatch(fh, key)
        state_stream.put((key, value))
    def add_watcher(inner_fh, watcher, fd_int, events, watcher_args):
        if events & 4 == 4:
            raise RuntimeError("IOError detected in event reactor")
        try:
            while True:
                tup = watch_stream.get(False)
                if tup == 'DIE':
                    fh.reactor_stop(fh.get_reactor())
                    break
                (job_id, tasks) = tup
                kvs.watch(fh, 'lwj.{}.state'.format(job_id), job_state_change, None)
                # TODO: generalize IO handling
                for task_id in range(0, tasks):
                    prefix = None
                    if args.prefix:
                        prefix = 'lwj.{}.{}: '.format(job_id, task_id)
                    kz.attach(fh, 'lwj.{}.{}.stdout'.format(job_id, task_id), sys.stdout, prefix)
                    kz.attach(fh, 'lwj.{}.{}.stderr'.format(job_id, task_id), sys.stderr, prefix)

                watch_stream.task_done()
        except Queue.Empty:
            pass
    with flux.core.watchers.FDWatcher(fh, watch_stream._reader.fileno(), 3, add_watcher, watch_stream):
        fh.reactor_run(fh.get_reactor(), 0)


if __name__ == "__main__":
    job_stream = mp.Queue(100)
    state_stream = mp.Queue(10000)
    watcher_stream = mp.JoinableQueue(100)
    finish = mp.Queue(1)

    reactor_proc = mp.Process(target=event_reactor_proc, args=(watcher_stream, state_stream))
    reactor_proc.start()
    sched_proc = mp.Process(target=scheduler_proc, args=(job_stream, watcher_stream, state_stream, finish))
    sched_proc.start()
    command_proc = mp.Process(target=command_supplier_proc, args=(job_stream,))
    command_proc.start()

    command_proc.join()
    sched_proc.join()
    reactor_proc.join()

